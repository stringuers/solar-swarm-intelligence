{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔍 Notebook 04: Anomaly Detection\n",
    "\n",
    "**Solar Swarm Intelligence - IEEE PES Energy Utopia Challenge**\n",
    "\n",
    "Detect faults and anomalies in solar panel systems:\n",
    "- Isolation Forest for outlier detection\n",
    "- Autoencoder neural network\n",
    "- Real-time fault identification\n",
    "- Performance degradation detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\" Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/synthetic/community_90days.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features for anomaly detection\n",
    "df['production_per_temp'] = df['production_kwh'] / (df['temperature_c'] + 1)\n",
    "df['production_cloud_ratio'] = df['production_kwh'] / (100 - df['cloud_cover_pct'] + 1)\n",
    "df['efficiency'] = df['production_kwh'] / (df['temperature_c'] * 0.1 + 1)\n",
    "\n",
    "# Select features\n",
    "feature_cols = ['production_kwh', 'temperature_c', 'cloud_cover_pct', \n",
    "                'humidity_pct', 'wind_speed_kmh', 'production_per_temp',\n",
    "                'production_cloud_ratio', 'efficiency']\n",
    "\n",
    "# Filter daytime hours only (6-18)\n",
    "daytime_df = df[(df['hour'] >= 6) & (df['hour'] <= 18)].copy()\n",
    "\n",
    "print(f\"Daytime data shape: {daytime_df.shape}\")\n",
    "print(f\"Features: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inject Synthetic Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create anomalies for testing (5% of data)\n",
    "np.random.seed(42)\n",
    "anomaly_indices = np.random.choice(daytime_df.index, size=int(len(daytime_df)*0.05), replace=False)\n",
    "\n",
    "# Create labels\n",
    "daytime_df['is_anomaly'] = 0\n",
    "daytime_df.loc[anomaly_indices, 'is_anomaly'] = 1\n",
    "\n",
    "# Inject anomalies (reduce production significantly)\n",
    "for idx in anomaly_indices:\n",
    "    anomaly_type = np.random.choice(['low_production', 'zero_production', 'erratic'])\n",
    "    \n",
    "    if anomaly_type == 'low_production':\n",
    "        daytime_df.loc[idx, 'production_kwh'] *= 0.3  # 70% reduction\n",
    "    elif anomaly_type == 'zero_production':\n",
    "        daytime_df.loc[idx, 'production_kwh'] = 0\n",
    "    else:  # erratic\n",
    "        daytime_df.loc[idx, 'production_kwh'] *= np.random.uniform(0.1, 0.4)\n",
    "\n",
    "# Recalculate derived features\n",
    "daytime_df['production_per_temp'] = daytime_df['production_kwh'] / (daytime_df['temperature_c'] + 1)\n",
    "daytime_df['production_cloud_ratio'] = daytime_df['production_kwh'] / (100 - daytime_df['cloud_cover_pct'] + 1)\n",
    "daytime_df['efficiency'] = daytime_df['production_kwh'] / (daytime_df['temperature_c'] * 0.1 + 1)\n",
    "\n",
    "print(f\"Total samples: {len(daytime_df)}\")\n",
    "print(f\"Normal samples: {(daytime_df['is_anomaly']==0).sum()}\")\n",
    "print(f\"Anomalies: {(daytime_df['is_anomaly']==1).sum()}\")\n",
    "print(f\"Anomaly rate: {(daytime_df['is_anomaly']==1).sum()/len(daytime_df)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = daytime_df[feature_cols].values\n",
    "y_true = daytime_df['is_anomaly'].values\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train Isolation Forest\n",
    "print(\" Training Isolation Forest...\\n\")\n",
    "iso_forest = IsolationForest(\n",
    "    contamination=0.05,\n",
    "    n_estimators=100,\n",
    "    max_samples=256,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "iso_forest.fit(X_scaled)\n",
    "print(\" Training complete!\")\n",
    "\n",
    "# Predict (-1 for anomalies, 1 for normal)\n",
    "iso_predictions = iso_forest.predict(X_scaled)\n",
    "iso_predictions_binary = (iso_predictions == -1).astype(int)\n",
    "\n",
    "# Anomaly scores\n",
    "iso_scores = iso_forest.score_samples(X_scaled)\n",
    "daytime_df['iso_anomaly_score'] = iso_scores\n",
    "daytime_df['iso_prediction'] = iso_predictions_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Isolation Forest\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "print(\"📊 ISOLATION FOREST PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {accuracy_score(y_true, iso_predictions_binary):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_true, iso_predictions_binary):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_true, iso_predictions_binary):.4f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_true, iso_predictions_binary):.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, iso_predictions_binary)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Isolation Forest - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Autoencoder Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim=8):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(4, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "print(\" Autoencoder model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on normal data only\n",
    "normal_data = daytime_df[daytime_df['is_anomaly'] == 0][feature_cols].values\n",
    "normal_scaled = scaler.transform(normal_data)\n",
    "\n",
    "# Convert to tensor\n",
    "X_train_ae = torch.FloatTensor(normal_scaled)\n",
    "X_test_ae = torch.FloatTensor(X_scaled)\n",
    "\n",
    "# Initialize model\n",
    "autoencoder = Autoencoder(input_dim=len(feature_cols))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "print(\" Training Autoencoder...\\n\")\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    autoencoder.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Mini-batch training\n",
    "    for i in range(0, len(X_train_ae), batch_size):\n",
    "        batch = X_train_ae[i:i+batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(batch)\n",
    "        loss = criterion(outputs, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(X_train_ae):.6f}\")\n",
    "\n",
    "print(\"\\n Autoencoder training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate reconstruction error\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructed = autoencoder(X_test_ae)\n",
    "    reconstruction_errors = torch.mean((X_test_ae - reconstructed) ** 2, dim=1).numpy()\n",
    "\n",
    "daytime_df['ae_reconstruction_error'] = reconstruction_errors\n",
    "\n",
    "# Set threshold (95th percentile of normal data errors)\n",
    "normal_errors = reconstruction_errors[y_true == 0]\n",
    "threshold = np.percentile(normal_errors, 95)\n",
    "\n",
    "print(f\"Anomaly threshold: {threshold:.6f}\")\n",
    "\n",
    "# Predict anomalies\n",
    "ae_predictions = (reconstruction_errors > threshold).astype(int)\n",
    "daytime_df['ae_prediction'] = ae_predictions\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n AUTOENCODER PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {accuracy_score(y_true, ae_predictions):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_true, ae_predictions):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_true, ae_predictions):.4f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_true, ae_predictions):.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm_ae = confusion_matrix(y_true, ae_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_ae, annot=True, fmt='d', cmap='Greens', cbar=False)\n",
    "plt.title('Autoencoder - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot anomaly scores\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Sample one house for visualization\n",
    "house_sample = daytime_df[daytime_df['house_id'] == 0].iloc[:500]\n",
    "\n",
    "# Isolation Forest scores\n",
    "axes[0].scatter(range(len(house_sample)), house_sample['iso_anomaly_score'], \n",
    "                c=house_sample['is_anomaly'], cmap='RdYlGn', s=30, alpha=0.6)\n",
    "axes[0].set_title('Isolation Forest Anomaly Scores (House 0)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Anomaly Score')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Autoencoder reconstruction errors\n",
    "axes[1].scatter(range(len(house_sample)), house_sample['ae_reconstruction_error'], \n",
    "                c=house_sample['is_anomaly'], cmap='RdYlGn', s=30, alpha=0.6)\n",
    "axes[1].axhline(y=threshold, color='red', linestyle='--', label=f'Threshold: {threshold:.4f}')\n",
    "axes[1].set_title('Autoencoder Reconstruction Error (House 0)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Sample Index')\n",
    "axes[1].set_ylabel('Reconstruction Error')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real-Time Anomaly Detection Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate real-time detection\n",
    "def detect_anomaly_realtime(production, temperature, cloud_cover, humidity, wind_speed):\n",
    "    \"\"\"Real-time anomaly detection function\"\"\"\n",
    "    \n",
    "    # Calculate derived features\n",
    "    prod_per_temp = production / (temperature + 1)\n",
    "    prod_cloud_ratio = production / (100 - cloud_cover + 1)\n",
    "    efficiency = production / (temperature * 0.1 + 1)\n",
    "    \n",
    "    # Create feature vector\n",
    "    features = np.array([[production, temperature, cloud_cover, humidity, wind_speed,\n",
    "                         prod_per_temp, prod_cloud_ratio, efficiency]])\n",
    "    \n",
    "    # Scale\n",
    "    features_scaled = scaler.transform(features)\n",
    "    \n",
    "    # Isolation Forest prediction\n",
    "    iso_pred = iso_forest.predict(features_scaled)[0]\n",
    "    iso_score = iso_forest.score_samples(features_scaled)[0]\n",
    "    \n",
    "    # Autoencoder prediction\n",
    "    with torch.no_grad():\n",
    "        features_tensor = torch.FloatTensor(features_scaled)\n",
    "        reconstructed = autoencoder(features_tensor)\n",
    "        ae_error = torch.mean((features_tensor - reconstructed) ** 2).item()\n",
    "    \n",
    "    ae_pred = 1 if ae_error > threshold else 0\n",
    "    \n",
    "    # Ensemble decision (both models agree)\n",
    "    is_anomaly = (iso_pred == -1) and (ae_pred == 1)\n",
    "    \n",
    "    return {\n",
    "        'is_anomaly': is_anomaly,\n",
    "        'iso_score': iso_score,\n",
    "        'ae_error': ae_error,\n",
    "        'confidence': 'HIGH' if (iso_pred == -1) and (ae_pred == 1) else 'MEDIUM' if (iso_pred == -1) or (ae_pred == 1) else 'LOW'\n",
    "    }\n",
    "\n",
    "# Test with examples\n",
    "print(\"🔍 REAL-TIME ANOMALY DETECTION EXAMPLES\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Normal case\n",
    "result1 = detect_anomaly_realtime(3.5, 25, 20, 60, 5)\n",
    "print(\"Test 1 - Normal Operation:\")\n",
    "print(f\"  Production: 3.5 kWh, Temp: 25°C, Cloud: 20%\")\n",
    "print(f\"  Anomaly: {result1['is_anomaly']}, Confidence: {result1['confidence']}\")\n",
    "print(f\"  ISO Score: {result1['iso_score']:.4f}, AE Error: {result1['ae_error']:.6f}\\n\")\n",
    "\n",
    "# Anomaly case\n",
    "result2 = detect_anomaly_realtime(0.5, 28, 15, 55, 6)\n",
    "print(\"Test 2 - Low Production (Potential Fault):\")\n",
    "print(f\"  Production: 0.5 kWh, Temp: 28°C, Cloud: 15%\")\n",
    "print(f\"  Anomaly: {result2['is_anomaly']}, Confidence: {result2['confidence']}\")\n",
    "print(f\"  ISO Score: {result2['iso_score']:.4f}, AE Error: {result2['ae_error']:.6f}\\n\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save Isolation Forest\n",
    "with open('../models/isolation_forest.pkl', 'wb') as f:\n",
    "    pickle.dump(iso_forest, f)\n",
    "print(\" Isolation Forest saved\")\n",
    "\n",
    "# Save Autoencoder\n",
    "torch.save(autoencoder.state_dict(), '../models/autoencoder.pth')\n",
    "print(\" Autoencoder saved\")\n",
    "\n",
    "# Save scaler\n",
    "with open('../models/anomaly_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\" Scaler saved\")\n",
    "\n",
    "# Save threshold\n",
    "with open('../models/ae_threshold.pkl', 'wb') as f:\n",
    "    pickle.dump(threshold, f)\n",
    "print(\" Threshold saved\")\n",
    "\n",
    "print(\"\\n All anomaly detection models saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "**Anomaly Detection Complete! ✅**\n",
    "\n",
    "**Implemented Methods:**\n",
    "1. **Isolation Forest**: Efficient outlier detection\n",
    "2. **Autoencoder**: Deep learning reconstruction-based detection\n",
    "\n",
    "**Key Capabilities:**\n",
    "- Real-time fault detection\n",
    "- Performance degradation identification\n",
    "- Panel malfunction alerts\n",
    "- Ensemble approach for higher accuracy\n",
    "\n",
    "**Applications:**\n",
    "- Predictive maintenance\n",
    "- System health monitoring\n",
    "- Early fault warning\n",
    "- Quality assurance\n",
    "\n",
    "**Next Steps:**\n",
    "- Notebook 05: Multi-agent swarm simulation integrating all models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
